{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "This project is the 4th project requirement in the Udacity Data Analyst Nanodegree program. In this project, I gather Twitter Rating Data from multiple sources as provided by Udacity, assessed the quality of the data collected, noted the issues and cleaned the data accordingly. \n",
    "\n",
    "In addition to storing the cleaned data, I also analyzed a portion of this data and generated some insights from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering dataset\n",
    "\n",
    "Gathering data is the process of collecting dataset to be analyzed. In this project, I gathered my data from the following three main channel:\n",
    "\n",
    "- Enhanced Twitter Archive: I was provided this data as a .csv file, so I initially read the file. This file contained information on tweeterâ€™s ID as well as rating information and dog stage.\n",
    "\n",
    "\n",
    "- Tweet Image Predictions: I downloaded this data as a .tsv from a URL with the Requests library. This file contained information on the image predictions of the breed of dog (or other objects) in each tweet.\n",
    "\n",
    "\n",
    "- WeRateDogs Twitter Archive: I created a new data frame by, firstly, querying the Twitter API for each tweet in the Twitter archive and saving JSON in a .txt file and then, using pandas, creating the data frame by extracting tweet_id, retweets, and favorites data from the .txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing dataset\n",
    "\n",
    "\n",
    "First, I visually assessed the three data frames listed above for any issues. I then used some common programmatic assessments in pandas to find any further issues. Every issue I found was summarized into two broad categories - Quality Issues or Tidiness Issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quality Issues\n",
    "\n",
    "There were several quality issues I had to contend with including the folowing:\n",
    "\n",
    "1. Missing values in 'name' and dog stages showing as 'None'\n",
    "1. rows with null values in expanded_uris column have no images\n",
    "2. incorrect data type for the timestamp column\n",
    "3. HTML tags are present in source \n",
    "4. Fix rating numerator and denominators that are not ratings\n",
    "5. Fixing rating numerator that have decimals\n",
    "6. getting dogs gender column from text column\n",
    "7. records that have a None placeholder do not have a name in the text column\n",
    "8. dog_category has the wrong data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidiness Issues\n",
    "\n",
    "Below are the tidiness issues I had to make ammendments to:\n",
    "\n",
    "1. twitter_archive and tweet_info should be combined\n",
    "1. duplicated data in twitter_archieve\n",
    "2. unnecessary columns in form of: doggo, floffer, pupper, puppo. \n",
    "3. image_predictions should be a part of twitter_archive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning dataset\n",
    "\n",
    "To clean the data, I began by using the three step cleaning process - Define, Code and Clean to get this done. The following are some examples of what I did to get the dataset cleaned:\n",
    "\n",
    "\n",
    "1. rows with null values in expanded_urls column have no images: to clean this, I queried out then dropped all null values in the expanded_urls columns using the notnull() pandas feature.\n",
    "\n",
    "\n",
    "2. incorrect data type for the timestamp column: to clean this, I used the to_datetime() pandas function to convert data type to timedate data type\n",
    "\n",
    "\n",
    "3. HTML tags are present in source: to clean this, I used the replace() functions in pandas to eliminate all HTML tags present in source column.\n",
    "\n",
    "\n",
    "4. Fix rating numerator and denominators that are not ratings: to fix this, I invoked the text.str() pandas function to filter the numerators and denominators that are not ratings\n",
    "\n",
    "\n",
    "5. Fixing rating numerator that have decimals: to clean this, I used pandas text.str() function to correct all numerators that have decimals.\n",
    "\n",
    "\n",
    "6. getting dogs gender column from text column: to clean this, I extracted male and female gender dogs by exracting all possible male and female gender pronouns then using loops feature.\n",
    "\n",
    "\n",
    "7. records that have a None placeholder do not have a name in the text column: to clean this, I invoked the pandas rename() function to change data values from \"None\" placeholders to actual null values (NaN in pandas).\n",
    "\n",
    "\n",
    "8. dog_category has the wrong data type: to clean this, I converted the data type using the astype() pandas function to change from object to category data type\n",
    "\n",
    "9. twitter_archive and tweet_info should be combined: to clean this, I merged both dataframes using inner merge. Like an SQL join function, the inner merge will not include retweet data and deleted tweets from fav_ret_counts table and twitter_archive respectively. This is good.\n",
    "\n",
    "10. duplicated data in twitter_archieve: to clean this, I used the isnul() function on the twitter_archive_clean dataframe then dropped all duplicates with the drop_duplicates() function.\n",
    "\n",
    "\n",
    "11. unnecessary columns in form of: doggo, floffer, pupper, puppo: to clean this, I merged all four columns into one column called - dog_category using pandas merge() function then proceeded to drop the extra four columns using pandas drop() function.\n",
    "\n",
    "\n",
    "12. image_predictions should be a part of twitter_archive: to clean this, I merged image_predictions with twitter_archive using pandas pd.merge() function.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
